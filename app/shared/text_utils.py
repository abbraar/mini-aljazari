# app/shared/text_utils.py
from __future__ import annotations

import os
import re
import logging
from typing import Dict, List, Optional, Tuple

# ---------------- Logging setup ----------------
logger = logging.getLogger(__name__)
if not logger.handlers:  # avoid duplicate handlers in reloads
    _handler = logging.StreamHandler()
    _handler.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s:%(name)s: %(message)s"))
    logger.addHandler(_handler)
# default WARNING; override with env: LOG_LEVEL=DEBUG (INFO, ERROR, etc.)
logger.setLevel(getattr(logging, os.getenv("LOG_LEVEL", "WARNING").upper(), logging.WARNING))

# ---------------- Optional dependencies (guarded) ----------------
try:
    import arabicprocess as ap  # stopwords / stemming
    logger.debug("arabicprocess loaded successfully.")
except Exception as e:
    ap = None
    logger.warning("arabicprocess not available: %s", e)

try:
    import pyarabic.araby as araby  # digit normalization, tashkeel utils
    logger.debug("pyarabic loaded successfully.")
except Exception as e:
    araby = None
    logger.warning("pyarabic not available: %s", e)

# ---------------- Regexes ----------------
_DIAC = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670]")  # harakat incl. 0670
_TATWEEL = re.compile(r"\u0640+")                           # Ù€Ù€Ù€
_PUNCT = re.compile(r"[^\w\s\u0600-\u06FF]")                # strip non-word non-Arabic punct
_MULTI_SPACE = re.compile(r"\s+")
_AR_WORDS = re.compile(r"[\u0600-\u06FF]{3,}")              # (if needed elsewhere)

def _strip_diacritics(s: str) -> str:
    return _DIAC.sub("", s)

def _basic_orthographic_norm(s: str) -> str:
    # widely used Arabic IR unifications
    s = s.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    s = s.replace("Ù‰", "ÙŠ").replace("Ø¦", "ÙŠ").replace("Ø¤", "Ùˆ")
    return s

def _collapse_ws(s: str) -> str:
    return _MULTI_SPACE.sub(" ", s).strip()

# Translate Arabic-Indic and eastern Arabic-Indic digits to ASCII 0-9
_ARABIC_INDIC_DIGITS = "Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©"  # U+0660..U+0669
_EASTERN_ARABIC_INDIC_DIGITS = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹"  # U+06F0..U+06F9 (Persian/Urdu forms)
_ASCII_DIGITS = "0123456789"
_DIGIT_TRANSLATION = str.maketrans({
    **{ord(a): ord(b) for a, b in zip(_ARABIC_INDIC_DIGITS, _ASCII_DIGITS)},
    **{ord(a): ord(b) for a, b in zip(_EASTERN_ARABIC_INDIC_DIGITS, _ASCII_DIGITS)},
})

def _to_ascii_digits(s: str) -> str:
    """Normalize Arabic-Indic digits (e.g., Ù Ù¡Ù¢) and eastern forms (Û°Û±Û²) to ASCII (012)."""
    return s.translate(_DIGIT_TRANSLATION)

def norm_ar(s: str) -> str:
    """
    Light, SAFE normalization.
    Use for:
      - dialect classification
      - theme rules
      - display/debug
    """
    if not s:
        return ""
    try:
        s = s.strip()
        s = _strip_diacritics(s)
        s = _basic_orthographic_norm(s)
        s = _collapse_ws(s)
        return s
    except Exception as e:
        logger.error("norm_ar failed: %s", e)
        return (s or "").strip()

def norm_ar_index(
    s: str,
    *,
    remove_tatweel: bool = True,
    normalize_digits: bool = True,
    strip_punct: bool = True,
    stopwords: bool = True,
    stem: bool = False,
) -> str:
    """
    Heavier, INDEX-ORIENTED normalization.
    Use ONLY for retrieval indexing/query (TF-IDF / dense encoders).
    Do NOT use for dialect/theme classification.
    """
    if not s:
        return ""
    try:
        t = norm_ar(s)  # start from safe normalizer

        if remove_tatweel:
            t = _TATWEEL.sub("", t)

        if normalize_digits:
            try:
                # Prefer pyarabic if it exposes normalize_digits; otherwise use local mapping
                if araby is not None and hasattr(araby, "normalize_digits"):
                    t = araby.normalize_digits(t)  # type: ignore[attr-defined]
                else:
                    t = _to_ascii_digits(t)
            except Exception as e:
                logger.error("Digit normalization failed: %s", e)

        if strip_punct:
            t = _PUNCT.sub(" ", t)

        # Optional utilities from arabicprocess (guarded)
        if ap is not None:
            if stopwords:
                try:
                    t = ap.remove_stopwords(t)
                except Exception as e:
                    logger.error("Stopword removal failed: %s", e)
            if stem:
                try:
                    t = ap.stem_text(t)  # careful: may hurt poetry precision
                except Exception as e:
                    logger.error("Stemming failed: %s", e)

        return _collapse_ws(t)
    except Exception as e:
        logger.error("norm_ar_index failed: %s", e)
        return _collapse_ws(s)

# ---------------- Theme rules (priority matters; first match wins) ----------------
THEME_RULES: Dict[str, List[str]] = {
    "ØºØ²Ù„": [
        "Ø­Ø¨", "Ø­Ø¨ÙŠØ¨", "Ø­Ø¨ÙŠØ¨ØªÙŠ", "Ø­Ø¨ÙŠØ¨ÙŠ", "Ø¹Ø´Ù‚", "ØºØ±Ø§Ù…", "Ù‡ÙˆÙ‰", "ÙˆÙ„Ù‡", "Ø´ÙˆÙ‚",
        "Ø§Ø´ØªÙ‚Øª", "Ø§Ù‡ÙˆØ§Ùƒ", "Ø§Ø¹Ø´Ù‚Ùƒ", "ÙˆØ¯", "ÙˆØµØ§Ù„", "ØºÙ„Ø§", "Ø¹ÙŠÙˆÙ†Ùƒ", "Ø¹ÙŠÙ†Ùƒ",
        "Ø¬Ù…Ø§Ù„", "Ø®Ø¯ÙˆØ¯", "Ù‚Ø¨Ù„", "Ø´ÙØ§ÙŠÙ", "Ø¶Ø­ÙƒØªÙƒ", "Ø®Ø¬Ù„Ùƒ", "Ø³Ù‡Ø±Ø§Ù†", "Ø­Ù†ÙŠÙ†",
        "Ù„Ù‚Ø§Ù†Ø§", "ÙˆØµÙ„Ùƒ", "Ù‡ÙˆØ§Ùƒ", "ÙˆØµØ§Ù„Ùƒ", "ØªØºØ²Ù„", "ØºØ²Ù„ÙŠ", "Ø§Ù„Ù‡ÙŠØ§Ù…", "Ø§Ù„ÙˆÙ„Ù‡Ø§Ù†",
        "Ø§Ø­Ø³Ø§Ø³Ùƒ", "Ø§Ø­Ø³Ø§Ø³"
    ],
    "ÙˆØ·Ù†ÙŠØ©": [
        "Ø³Ø¹ÙˆØ¯ÙŠ", "Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø§Ù„ÙˆØ·Ù†", "ÙˆØ·Ù†", "Ø¨Ù„Ø§Ø¯", "Ù…ÙˆØ·Ù†ÙŠ", "Ø±Ø§ÙŠØ©", "Ø§Ù„Ø¹Ù„Ù…", "Ø§Ù„Ù…Ù„Ùƒ",
        "Ø³Ù„Ù…Ø§Ù†", "ÙˆØ­Ø¯Ø©", "ØªÙˆØ­Ø¯", "Ø¨ÙŠØ¹Ø©", "Ø¯Ø§Ø±", "Ø¬Ø²ÙŠØ±Ø©", "Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ²", "Ø¹Ø¨Ø¯ Ø§Ù„Ø¹Ø²ÙŠØ²",
        "Ù…Ø¬Ø¯ÙŠ", "ÙØ®Ø±", "Ø¨Ù„Ø§Ø¯ÙŠ", "ØªØ±Ø§Ø¨Ù‡Ø§", "ÙˆØ·Ù†Ù†Ø§", "Ø§Ù„Ù…Ù…Ù„ÙƒØ©", "Ù†Ù‡Ø¶Ø©", "Ø±Ø¤ÙŠØ©", "2030",
        "ÙˆØ·Ù† Ø§Ù„Ø¹Ø²", "Ø¯Ø§Ø±Ù†Ø§", "Ø­ÙƒØ§Ù…Ù†Ø§", "Ø§Ù„Ø¹Ù„Ù… Ø§Ù„Ø£Ø®Ø¶Ø±", "Ø§Ù„Ù…Ø¬Ø¯", "Ø±Ø§ÙØ¹", "Ø±Ø§ÙŠØªÙ†Ø§"
    ],
    "Ø±ÙŠØ§Ø¶ÙŠØ©": [
        "Ù†Ø§Ø¯ÙŠ", "Ø§Ù„Ø§ØªØ­Ø§Ø¯", "Ø§Ù„Ø§ØªÙŠ", "Ø§Ù„Ù‡Ù„Ø§Ù„", "Ø§Ù„Ø§Ù‡Ù„ÙŠ", "Ø§Ù„Ù†ØµØ±", "Ø§Ù„Ø´Ø¨Ø§Ø¨", "Ø§Ù„ÙÙŠØµÙ„ÙŠ",
        "Ø§Ù„Ø§ØªÙØ§Ù‚", "Ø§Ù„Ø·Ø§Ø¦ÙŠ", "Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©", "Ù…Ù„Ø¹Ø¨", "Ù…Ø¯Ø±Ø¬", "Ø¬Ù…Ù‡ÙˆØ±", "ØªØ´Ø¬ÙŠØ¹", "Ù…Ø¨Ø§Ø±Ø§Ø©",
        "Ø¨Ø·ÙˆÙ„Ø©", "Ø¯ÙˆØ±ÙŠ", "ÙƒØ§Ø³", "ÙƒØ£Ø³", "Ù…Ø¯Ø±Ø¨", "Ù„Ø§Ø¹Ø¨", "Ù‡Ø¯Ù", "Ø±ÙƒÙ„Ø©", "Ø´ÙˆØ·", "ØªØ³Ø¯ÙŠØ¯Ø©",
        "ÙÙˆØ²", "Ù‡Ø²ÙŠÙ…Ø©", "ØªØ¹Ø§Ø¯Ù„", "Ø­ÙƒÙ…", "ØªØ¨Ø¯ÙŠÙ„", "Ù…ÙŠØ¯Ø§Ù†", "ÙƒØ±Ø©"
    ],
    "Ø¯ÙŠÙ†ÙŠØ©": [
        "Ø§Ù„Ù„Ù‡", "Ø§Ù„Ù„Ù‡Ù…", "Ø³Ø¨Ø­Ø§Ù†", "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡", "Ø§Ø³ØªØºÙØ± Ø§Ù„Ù„Ù‡", "ÙŠØ§Ø±Ø¨", "ÙŠØ§ Ø±Ø¨", "Ø±Ø¨ÙŠ",
        "Ø±Ø¨Ù†Ø§", "Ù†Ø¨ÙŠ", "Ø±Ø³ÙˆÙ„", "Ù…Ø­Ù…Ø¯", "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "Ù…ÙƒØ©", "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„Ù…Ø¯ÙŠÙ†Ù‡",
        "Ø­Ø¬", "Ø¹Ù…Ø±Ø©", "Ø¯Ø¹Ø§Ø¡", "Ø§ÙŠÙ…Ø§Ù†", "ØªÙˆØ­ÙŠØ¯", "Ù…Ø³Ø¬Ø¯", "Ù‚Ø¨Ù„Ø©", "Ù‚Ø±Ø¢Ù†", "Ø³Ù†Ø©", "Ø­Ø¯ÙŠØ«",
        "ØµÙ„Ø§Ø©", "Ø§Ù„ØµÙ„Ø§Ø©", "Ø²ÙƒØ§Ø©", "ØµÙŠØ§Ù…", "Ø±Ù…Ø¶Ø§Ù†", "Ø§Ù„Ø¬Ù…Ø¹Ø©", "Ù‚ÙŠØ§Ù… Ø§Ù„Ù„ÙŠÙ„", "Ø¬Ù†Ø©", "Ù†Ø§Ø±",
        "Ø¹Ø°Ø§Ø¨", "Ø¢Ø®Ø±Ø©", "ØªÙˆØ¨Ø©", "Ø±Ø­Ù…Ø©", "Ø§Ø³ØªØºÙØ§Ø±", "ØªØ³Ø¨ÙŠØ­", "ØªÙ‡Ù„ÙŠÙ„", "ØªÙƒØ¨ÙŠØ±", "ØªØ­Ù…ÙŠØ¯"
    ],
    "Ø´ÙˆÙ‚": [
        "Ø§Ø´ØªØ§Ù‚", "Ù…Ø´ØªØ§Ù‚", "Ø£Ø´ØªØ§Ù‚", "Ø£ÙˆÙ„Ù‡", "Ø­Ù†ÙŠÙ†", "ØªØ°ÙƒØ±", "ØªØ°ÙƒØ±Øª", "Ø±Ø¬Ø¹Ø©", "ØºØ¨Øª",
        "ØºÙŠØ§Ø¨Ùƒ", "ÙˆØ¯Ø§Ø¹", "Ø±Ø¬ÙˆØ¹", "Ù…Ø§ Ù†Ø³ÙŠØªÙƒ", "Ø¨Ø¹Ø¯Ùƒ", "Ø·ÙŠÙÙƒ", "ØºÙŠØ¨ØªÙƒ"
    ],
    "Ø­Ø²Ù†": [
        "ÙØ±Ø§Ù‚", "ÙˆØ¯Ø§Ø¹", "Ø±Ø­ÙŠÙ„", "Ø¯Ù…Ø¹", "Ø¯Ù…ÙˆØ¹ÙŠ", "Ø¨ÙƒÙŠØª", "Ø¬Ø±ÙˆØ­", "Ø­Ø²Ù†", "ÙƒØ³Ø±", "ÙˆØ¬Ø¹",
        "ØºØ¨Øª", "ØºÙŠØ§Ø¨", "ÙˆØ­Ø¯ÙŠ", "Ø¶ÙŠØ§Ø¹", "Ù…Ø£Ø³Ø§Ø©"
    ],
    "Ø¹ØªØ§Ø¨": [
        "ØºØ¯Ø±Øª", "Ø®ÙŠØ§Ù†Ø©", "Ø¬ÙÙŠØª", "Ù†Ø³ÙŠØª", "Ù‚ØµØ±Øª", "Ø²Ø¹Ù„Øª", "Ø¸Ù„Ù…ØªÙ†ÙŠ", "ØºØ¨Øª Ø¹Ù†ÙŠ",
        "Ù…Ø§ Ø³Ø£Ù„Øª", "Ù…Ø§ ÙˆÙÙŠØª", "ÙˆØ¹Ø¯Ùƒ", "ÙƒØ°Ø¨Øª", "Ø®Ø§Ù†", "ØªØ¬Ø§Ù‡Ù„Øª", "Ø¹Ø°Ø±Ùƒ", "Ù„ÙˆÙ…"
    ],
    "ÙØ®Ø±": [
        "ÙØ®Ø±", "Ø¹Ø²", "ÙƒØ±Ø§Ù…Ø©", "Ù†Ø®ÙˆØ©", "Ø´Ø¬Ø§Ø¹Ø©", "Ø³ÙŠÙ", "Ø³Ù„Ø§Ù„Ø©", "ÙƒØ±Ù…", "Ù…Ø±Ø¬Ù„Ø©",
        "ÙØ²Ø¹Ø©", "Ø±Ø¬Ø§Ù„", "Ø¨Ø·ÙˆÙ„Ø©", "Ù‚ÙˆØ©", "Ù…Ø±ÙˆØ¡Ø©", "Ù‡ÙŠØ¨Ø©"
    ],
    "ØªØ±Ø§Ø«": [
        "Ø®ÙŠÙ…Ø©", "Ù†Ø§Ù‚Ø©", "Ø±Ø¨Ø§Ø¨Ø©", "Ù‚Ù‡ÙˆØ©", "ÙÙ†Ø¬Ø§Ù„", "Ø§Ù„Ø¨Ø¯Ùˆ", "Ø§Ù„Ø¨Ø¯ÙˆÙŠ", "Ø¹Ù‚Ø§Ù„", "Ø´Ù…Ø§Øº",
        "ØµØ­Ø±Ù‰", "Ø±Ù…Ù„", "Ø¯Ù‡Ù† Ø§Ù„Ø¹ÙˆØ¯", "ÙØ§Ø±Ø³", "Ø¬Ù…Ù„", "ØªØ±Ø§Ø«", "Ø¹Ø±Ø¶Ø©", "Ù…Ø²Ù…Ø§Ø±"
    ],
    "Ø§Ø­ØªÙØ§Ù„": [
        "Ø²ÙØ©", "Ø²ÙˆØ§Ø¬", "Ø¹Ø±Ø³", "ÙØ±Ø­", "Ù„ÙŠÙ„Ø© Ø§Ù„Ø¹Ù…Ø±", "Ù…Ø¨Ø±ÙˆÙƒ", "Ù…Ù„ÙƒØ©", "ØªÙ‡Ù†Ø¦Ø©", "Ø·Ø¨ÙˆÙ„",
        "Ù…Ø²Ù…Ø§Ø±", "Ù†Ù‚ÙˆØ·", "Ù…Ù†Ø§Ø³Ø¨Ø§Øª", "Ø­ÙÙ„", "Ø­ÙÙ„Ø©", "Ø¹ÙŠØ¯", "Ø³Ø¹Ø§Ø¯Ø©"
    ],
    "Ø¨Ø­Ø±": [
        "Ø¨Ø­Ø±", "Ø´Ø§Ø·Ø¦", "Ù…ÙˆØ¬", "Ù…Ø±Ø³Ù‰", "ØµÙŠØ§Ø¯", "Ø´Ø±Ø§Ø¹", "Ù„Ø¤Ù„Ø¤", "ØºÙˆØµ", "Ù†Ø³ÙŠÙ…",
        "Ù‚Ø§Ø±Ø¨", "Ù…Ø±ÙƒØ¨", "Ø³ÙÙŠÙ†Ø©", "Ø¹Ø§ØµÙØ©", "Ù…Ù„Ø§Ø­", "Ø£Ù…ÙˆØ§Ø¬"
    ],
    "Ø·Ù…ÙˆØ­": [
        "Ø­Ù„Ù…", "Ø·Ù…ÙˆØ­", "Ø³Ù‡Ø±", "ØªØ¹Ø¨", "ÙƒÙØ§Ø­", "Ø§ØµØ±Ø§Ø±", "Ù†Ø¬Ø§Ø­", "Ø§Ù†Ø¬Ø§Ø²", "Ù‡Ø¯Ù",
        "Ø§Ù…Ù„", "Ø§ØµØ±Ø§Ø±ÙŠ", "Ø£ÙˆØ§ØµÙ„", "Ø£ØªØ­Ø¯Ù‰", "Ø£Ø­Ù‚Ù‚", "Ø§Ø¬ØªÙ‡Ø§Ø¯"
    ],
    "Ù…Ø±Ø­": [
        "Ø·Ø±Ø¨", "Ù†ØºÙ†ÙŠ", "Ù†Ø±Ù‚Øµ", "Ø³Ù‡Ø±", "Ø³Ù‡Ø±Ø©", "ÙØ±Ø­Ø©", "Ù…Ø²Ø§Ø¬", "Ù„ÙŠÙ„Ø©", "Ù†ØºÙ…",
        "Ù†ØºÙ…Ø©", "Ø¹Ø²Ù", "Ù…ÙˆØ³ÙŠÙ‚Ù‰", "Ù†ÙˆØªØ©", "ØªØµÙÙŠÙ‚", "Ø¨Ù‡Ø¬Ø©", "Ø¶Ø­Ùƒ"
    ],
    "Ø³ÙØ±": [
        "ØºØ±Ø¨Ø©", "Ø³ÙØ±", "Ø±Ø¬Ø¹Ø©", "Ø¨Ø¹ÙŠØ¯", "Ø£Ø±Ø¬Ø¹", "ØªØ°ÙƒØ±Ø©", "Ù…Ø·Ø§Ø±", "ÙˆØ¯Ø§Ø¹", "Ø£Ù‡Ù„",
        "Ø¯ÙŠØ±Ø©", "Ù…Ø¯ÙŠÙ†Ø©", "Ø±Ø¬Ø¹Øª", "Ø§Ø´ØªÙ‚Øª Ù„Ù„ÙˆØ·Ù†", "Ø¹ÙˆØ¯Ø©"
    ],
    "Ù†Ù‚Ø¯": [
        "Ø²Ù…Ù†", "Ø¯Ù†ÙŠØ§", "Ù†Ø§Ø³", "Ù…Ø¬ØªÙ…Ø¹", "Ø­Ø§Ù„Ù†Ø§", "Ø³Ø®Ø±ÙŠØ©", "Ù†ÙØ§Ù‚", "Ù‚Ù‡Ø±", "Ø¸Ù„Ù…",
        "Ø³Ø§Ù„ÙØ©", "Ø²Ù…Ø§Ù†", "Ø­ÙŠØ§Ø©", "ÙˆØ§Ù‚Ø¹", "ØªÙ‡ÙƒÙ…", "ØªØ¹Ø¨", "Ù‡Ù…"
    ],
    "Ø·Ø¨ÙŠØ¹Ø©": [
        "Ù…Ø·Ø±", "ØºÙŠÙ…", "Ø±Ø¹Ø¯", "Ø¨Ø±Ù‚", "Ù†Ø³ÙŠÙ…", "ÙˆØ±Ø¯", "Ø²Ù‡ÙˆØ±", "Ø±Ø¨ÙŠØ¹", "Ø±ÙˆØ¶Ø©",
        "Ø²Ù‡Ø±", "Ø£Ù…Ø·Ø§Ø±", "Ø³Ù…Ø§Ø¡", "Ø¹ØµØ§ÙÙŠØ±", "Ø­Ù‚Ù„", "Ù†Ø³Ù…Ø©", "ØºØ§Ø¨Ø©"
    ]
}


# --- Compile theme rules into Arabic-friendly token-boundary regexes ---
# We normalize each pattern with norm_ar so matching is consistent with input normalization.
def _compile_theme_rules(rules: Dict[str, List[str]]) -> Dict[str, List[re.Pattern]]:
    compiled: Dict[str, List[re.Pattern]] = {}
    for theme, pats in rules.items():
        compiled_list: List[re.Pattern] = []
        for p in pats:
            try:
                # Use LIGHT normalization for rule terms, then guard with whitespace boundaries
                # (?<!\S) and (?!\S) are robust for Arabic token delimiting without breaking on diacritics.
                norm_p = norm_ar(p)
                compiled_list.append(re.compile(rf"(?<!\S){re.escape(norm_p)}(?!\S)"))
            except Exception as e:
                logger.error("Failed compiling theme pattern '%s' for theme '%s': %s", p, theme, e)
        compiled[theme] = compiled_list
    return compiled

_COMPILED_THEME_RULES: Dict[str, List[re.Pattern]] = _compile_theme_rules(THEME_RULES)

def guess_theme_rules_with_match(text: str) -> Tuple[str, Optional[str], Optional[str]]:
    """
    Return (theme, matched_regex_or_literal, matched_span) using LIGHT-normalized text.
    Priority matters: first match wins.
    """
    t = norm_ar(text)
    for theme, patterns in _COMPILED_THEME_RULES.items():
        for rgx in patterns:
            m = rgx.search(t)
            if m:
                return theme, rgx.pattern, m.group(0)
    return "Ø§Ø®Ø±Ù‰", None, None

def tag_theme(text: str) -> str:
    theme, _, _ = guess_theme_rules_with_match(text)
    return theme

# Public symbols
__all__ = [
    "norm_ar", "norm_ar_index",
    "THEME_RULES",
    "guess_theme_rules_with_match", "tag_theme",
]

# ---------------- Quick self-test ----------------
if __name__ == "__main__":
    os.environ.setdefault("LOG_LEVEL", "DEBUG")
    logger.setLevel(logging.DEBUG)

    sample = "Ø£Ø­Ø¨ ÙˆØ·Ù†ÙŠ ğŸ‡¸ğŸ‡¦ ÙˆØ£Ù‡Ù„ÙŠ ÙƒØ«ÙŠØ±Ø§Ù‹!!! Ù Ù¡Ù¢"
    print("light :", norm_ar(sample))
    print("index :", norm_ar_index(sample))

    # Theme quick check
    ex1 = "Ø£Ø¹Ø´Ù‚Ùƒ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠ"
    ex2 = "Ø±Ø§ÙŠØ© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø®ÙØ§Ù‚Ø©"
    print("tag(ex1):", tag_theme(ex1), "match:", guess_theme_rules_with_match(ex1))
    print("tag(ex2):", tag_theme(ex2), "match:", guess_theme_rules_with_match(ex2))
